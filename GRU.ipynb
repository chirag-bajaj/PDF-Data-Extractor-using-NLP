{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "PUV2I7D_78Tm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers.core import Activation, Dense, Dropout, RepeatVector, SpatialDropout1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import GRU\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "import collections\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C_uTOP4M-jEf",
        "colab_type": "code",
        "outputId": "2999cf78-fc3e-4d0e-bf0e-8be305f78c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name=tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CzH9c0oG_QxR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11954
        },
        "outputId": "a2f6c46a-5d1d-4f61-8595-aca5fb424b36"
      },
      "cell_type": "code",
      "source": [
        "\n",
        " \n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('treebank')\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        " \n",
        "#print(tagged_sentences[0])\n",
        "#print(\"Tagged sentences: \", len(tagged_sentences))\n",
        "#print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))\n",
        " \n",
        "sentences, sentence_tags =[], [] \n",
        "for tagged_sentence in tagged_sentences:\n",
        "    sentence, tags = zip(*tagged_sentence)\n",
        "    sentences.append(np.array(sentence))\n",
        "    sentence_tags.append(np.array(tags))\n",
        "\n",
        "print(sentences[5])\n",
        "print(sentence_tags[5])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        " \n",
        "(train_sentences, test_sentences, train_tags, test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)\n",
        "\n",
        "words, tags = set([]), set([])\n",
        " \n",
        "for s in train_sentences:\n",
        "    for w in s:\n",
        "        words.add(w.lower())\n",
        " \n",
        "for ts in train_tags:\n",
        "    for t in ts:\n",
        "        tags.add(t)\n",
        " \n",
        "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0  # The special value used for padding\n",
        "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
        " \n",
        "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
        "tag2index['-PAD-'] = 0\n",
        "\n",
        "\n",
        "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        " \n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])\n",
        "\n",
        "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "print(MAX_LENGTH)\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "model.summary()\n",
        "\n",
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)\n",
        "  \n",
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "print(cat_train_tags_y[0])\n",
        "\n",
        "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)\n",
        "\n",
        "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
        "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")\n",
        "\n",
        "test_samples = [\n",
        "    \"running is very important for me .\".split(),\n",
        "    \"I was running every day for a month .\".split()\n",
        "]\n",
        "print(test_samples)\n",
        "\n",
        "test_samples_X = []\n",
        "for s in test_samples:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        "    test_samples_X.append(s_int)\n",
        " \n",
        "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "print(test_samples_X)\n",
        "\n",
        "predictions = model.predict(test_samples_X)\n",
        "print(predictions, predictions.shape)\n",
        "\n",
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])\n",
        " \n",
        "        token_sequences.append(token_sequence)\n",
        " \n",
        "    return token_sequence\n",
        "\n",
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))\n",
        "\n",
        "from keras import backend as K\n",
        " \n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
        " \n",
        "model.summary()\n",
        "\n",
        "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)\n",
        "\n",
        "predictions = model.predict(test_samples_X)\n",
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "model.summary()\n",
        "\n",
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)\n",
        "  \n",
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "print(cat_train_tags_y[0])\n",
        "\n",
        "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)\n",
        "\n",
        "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
        "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")\n",
        "\n",
        "test_samples = [\n",
        "    \"running is very important for me .\".split(),\n",
        "    \"I was running every day for a month .\".split()\n",
        "]\n",
        "print(test_samples)\n",
        "\n",
        "test_samples_X = []\n",
        "for s in test_samples:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        "    test_samples_X.append(s_int)\n",
        " \n",
        "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "print(test_samples_X)\n",
        "\n",
        "predictions = model.predict(test_samples_X)\n",
        "print(predictions, predictions.shape)\n",
        "\n",
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])\n",
        " \n",
        "        token_sequences.append(token_sequence)\n",
        " \n",
        "    return token_sequence\n",
        "\n",
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))\n",
        "\n",
        "from keras import backend as K\n",
        " \n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
        " \n",
        "model.summary()\n",
        "\n",
        "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)\n",
        "\n",
        "predictions = model.predict(test_samples_X)\n",
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "['Lorillard' 'Inc.' ',' 'the' 'unit' 'of' 'New' 'York-based' 'Loews'\n",
            " 'Corp.' 'that' '*T*-2' 'makes' 'Kent' 'cigarettes' ',' 'stopped' 'using'\n",
            " 'crocidolite' 'in' 'its' 'Micronite' 'cigarette' 'filters' 'in' '1956'\n",
            " '.']\n",
            "['NNP' 'NNP' ',' 'DT' 'NN' 'IN' 'JJ' 'JJ' 'NNP' 'NNP' 'WDT' '-NONE-' 'VBZ'\n",
            " 'NNP' 'NNS' ',' 'VBD' 'VBG' 'NN' 'IN' 'PRP$' 'NN' 'NN' 'NNS' 'IN' 'CD'\n",
            " '.']\n",
            "[9425, 9777, 1501, 9425, 2655, 525, 6004, 8480, 6358, 4675, 1841, 4802, 5189, 2418, 7686, 6400, 6963]\n",
            "[1622, 2207, 1, 7595, 1, 8625, 1501, 9058, 2121, 8806, 2897, 5940, 7559, 7595, 9470, 9425, 4306, 1501, 7739, 1, 2501, 186, 7837, 5619, 1898, 6963]\n",
            "[18, 46, 35, 18, 46, 19, 2, 13, 35, 18, 23, 2, 7, 29, 29, 13, 11]\n",
            "[30, 30, 30, 34, 38, 46, 35, 30, 30, 30, 35, 38, 30, 34, 21, 18, 46, 35, 18, 46, 42, 46, 35, 30, 29, 11]\n",
            "271\n",
            "[9425 9777 1501 9425 2655  525 6004 8480 6358 4675 1841 4802 5189 2418\n",
            " 7686 6400 6963    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[1622 2207    1 7595    1 8625 1501 9058 2121 8806 2897 5940 7559 7595\n",
            " 9470 9425 4306 1501 7739    1 2501  186 7837 5619 1898 6963    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[18 46 35 18 46 19  2 13 35 18 23  2  7 29 29 13 11  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n",
            "[30 30 30 34 38 46 35 30 30 30 35 38 30 34 21 18 46 35 18 46 42 46 35 30\n",
            " 29 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "['Lorillard' 'Inc.' ',' 'the' 'unit' 'of' 'New' 'York-based' 'Loews'\n",
            " 'Corp.' 'that' '*T*-2' 'makes' 'Kent' 'cigarettes' ',' 'stopped' 'using'\n",
            " 'crocidolite' 'in' 'its' 'Micronite' 'cigarette' 'filters' 'in' '1956'\n",
            " '.']\n",
            "['NNP' 'NNP' ',' 'DT' 'NN' 'IN' 'JJ' 'JJ' 'NNP' 'NNP' 'WDT' '-NONE-' 'VBZ'\n",
            " 'NNP' 'NNS' ',' 'VBD' 'VBG' 'NN' 'IN' 'PRP$' 'NN' 'NN' 'NNS' 'IN' 'CD'\n",
            " '.']\n",
            "[8787, 9465, 6143, 7884, 580, 190, 9273, 7760, 4814, 7605, 580, 5160, 7292, 9065, 8479, 8787, 9050, 8214, 3631, 7859, 36, 1513, 1903, 9452, 8787, 7393, 9452, 6399, 6968, 4657]\n",
            "[9465, 3587, 5602, 8810, 1, 10071, 8851, 1, 5730, 7760, 1858, 6968]\n",
            "[1, 20, 46, 13, 17, 23, 41, 20, 13, 34, 17, 8, 24, 38, 13, 1, 41, 20, 46, 35, 46, 35, 29, 46, 1, 29, 46, 13, 11, 16]\n",
            "[20, 38, 29, 30, 42, 19, 35, 42, 43, 20, 46, 11]\n",
            "271\n",
            "[8787 9465 6143 7884  580  190 9273 7760 4814 7605  580 5160 7292 9065\n",
            " 8479 8787 9050 8214 3631 7859   36 1513 1903 9452 8787 7393 9452 6399\n",
            " 6968 4657    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[ 9465  3587  5602  8810     1 10071  8851     1  5730  7760  1858  6968\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0]\n",
            "[ 1 20 46 13 17 23 41 20 13 34 17  8 24 38 13  1 41 20 46 35 46 35 29 46\n",
            "  1 29 46 13 11 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n",
            "[20 38 29 30 42 19 35 42 43 20 46 11  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 271, 128)          1296768   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 271, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 271, 47)           24111     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 271, 47)           0         \n",
            "=================================================================\n",
            "Total params: 2,109,359\n",
            "Trainable params: 2,109,359\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n",
            "Train on 2504 samples, validate on 627 samples\n",
            "Epoch 1/40\n",
            "2504/2504 [==============================] - 25s 10ms/step - loss: 1.3627 - acc: 0.8585 - val_loss: 0.3917 - val_acc: 0.9028\n",
            "Epoch 2/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.3441 - acc: 0.9139 - val_loss: 0.3226 - val_acc: 0.9141\n",
            "Epoch 3/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.3146 - acc: 0.9089 - val_loss: 0.3128 - val_acc: 0.9111\n",
            "Epoch 4/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.3052 - acc: 0.9162 - val_loss: 0.3052 - val_acc: 0.9177\n",
            "Epoch 5/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2970 - acc: 0.9175 - val_loss: 0.2974 - val_acc: 0.9179\n",
            "Epoch 6/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2888 - acc: 0.9177 - val_loss: 0.2923 - val_acc: 0.9176\n",
            "Epoch 7/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2830 - acc: 0.9183 - val_loss: 0.2904 - val_acc: 0.9178\n",
            "Epoch 8/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2820 - acc: 0.9193 - val_loss: 0.2811 - val_acc: 0.9214\n",
            "Epoch 9/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2740 - acc: 0.9223 - val_loss: 0.2736 - val_acc: 0.9216\n",
            "Epoch 10/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2690 - acc: 0.9240 - val_loss: 0.2702 - val_acc: 0.9256\n",
            "Epoch 11/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2649 - acc: 0.9274 - val_loss: 0.2651 - val_acc: 0.9301\n",
            "Epoch 12/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2602 - acc: 0.9323 - val_loss: 0.2605 - val_acc: 0.9368\n",
            "Epoch 13/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2542 - acc: 0.9381 - val_loss: 0.2538 - val_acc: 0.9397\n",
            "Epoch 14/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2455 - acc: 0.9409 - val_loss: 0.2438 - val_acc: 0.9413\n",
            "Epoch 15/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2336 - acc: 0.9431 - val_loss: 0.2302 - val_acc: 0.9440\n",
            "Epoch 16/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2185 - acc: 0.9470 - val_loss: 0.2143 - val_acc: 0.9498\n",
            "Epoch 17/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2017 - acc: 0.9521 - val_loss: 0.1979 - val_acc: 0.9522\n",
            "Epoch 18/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.1846 - acc: 0.9543 - val_loss: 0.1817 - val_acc: 0.9533\n",
            "Epoch 19/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.1681 - acc: 0.9567 - val_loss: 0.1670 - val_acc: 0.9562\n",
            "Epoch 20/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.1527 - acc: 0.9597 - val_loss: 0.1535 - val_acc: 0.9585\n",
            "Epoch 21/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.1380 - acc: 0.9634 - val_loss: 0.1405 - val_acc: 0.9614\n",
            "Epoch 22/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.1237 - acc: 0.9680 - val_loss: 0.1279 - val_acc: 0.9652\n",
            "Epoch 23/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.1097 - acc: 0.9718 - val_loss: 0.1158 - val_acc: 0.9689\n",
            "Epoch 24/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0964 - acc: 0.9757 - val_loss: 0.1045 - val_acc: 0.9725\n",
            "Epoch 25/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0840 - acc: 0.9795 - val_loss: 0.0938 - val_acc: 0.9766\n",
            "Epoch 26/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0723 - acc: 0.9833 - val_loss: 0.0841 - val_acc: 0.9792\n",
            "Epoch 27/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0617 - acc: 0.9864 - val_loss: 0.0750 - val_acc: 0.9817\n",
            "Epoch 28/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0524 - acc: 0.9888 - val_loss: 0.0673 - val_acc: 0.9840\n",
            "Epoch 29/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0444 - acc: 0.9908 - val_loss: 0.0610 - val_acc: 0.9854\n",
            "Epoch 30/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0377 - acc: 0.9922 - val_loss: 0.0571 - val_acc: 0.9864\n",
            "Epoch 31/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0323 - acc: 0.9933 - val_loss: 0.0526 - val_acc: 0.9873\n",
            "Epoch 32/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0281 - acc: 0.9941 - val_loss: 0.0483 - val_acc: 0.9882\n",
            "Epoch 33/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0245 - acc: 0.9948 - val_loss: 0.0465 - val_acc: 0.9886\n",
            "Epoch 34/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0216 - acc: 0.9954 - val_loss: 0.0452 - val_acc: 0.9891\n",
            "Epoch 35/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0192 - acc: 0.9960 - val_loss: 0.0437 - val_acc: 0.9895\n",
            "Epoch 36/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0172 - acc: 0.9964 - val_loss: 0.0412 - val_acc: 0.9899\n",
            "Epoch 37/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0156 - acc: 0.9968 - val_loss: 0.0405 - val_acc: 0.9901\n",
            "Epoch 38/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0142 - acc: 0.9971 - val_loss: 0.0401 - val_acc: 0.9902\n",
            "Epoch 39/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0130 - acc: 0.9973 - val_loss: 0.0397 - val_acc: 0.9903\n",
            "Epoch 40/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0120 - acc: 0.9975 - val_loss: 0.0387 - val_acc: 0.9905\n",
            "783/783 [==============================] - 10s 13ms/step\n",
            "acc: 98.97498931464564\n",
            "[['running', 'is', 'very', 'important', 'for', 'me', '.'], ['I', 'was', 'running', 'every', 'day', 'for', 'a', 'month', '.']]\n",
            "[[5532 6077 2986 6833 4641 2005 6968    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [6126  523 5532 4937    6 4641 8214 3962 6968    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]]\n",
            "[[[7.8357058e-04 6.0194780e-05 1.3336322e-02 ... 3.2091007e-06\n",
            "   6.0374271e-03 5.9844192e-02]\n",
            "  [1.3018060e-05 3.3412917e-07 4.6309787e-03 ... 9.8780539e-08\n",
            "   8.5472973e-04 1.0042111e-05]\n",
            "  [4.1795929e-04 8.5471737e-07 2.7860086e-02 ... 1.3589560e-06\n",
            "   1.8840447e-03 3.4637058e-06]\n",
            "  ...\n",
            "  [9.9997568e-01 2.6637773e-10 4.9652135e-06 ... 7.2538668e-09\n",
            "   5.2534510e-10 6.0773830e-10]\n",
            "  [9.9995637e-01 7.3077566e-10 5.6846911e-06 ... 1.8942997e-08\n",
            "   7.8513168e-10 4.5760087e-10]\n",
            "  [9.9992418e-01 1.6064846e-09 6.5356821e-06 ... 3.8041122e-08\n",
            "   1.1301586e-09 3.9944273e-10]]\n",
            "\n",
            " [[9.5335754e-06 2.5427534e-04 8.0960954e-06 ... 2.5289671e-07\n",
            "   3.3456793e-03 2.6423286e-04]\n",
            "  [1.5413631e-06 2.6718780e-10 2.8437902e-03 ... 1.9433655e-08\n",
            "   1.3111053e-04 2.0134820e-08]\n",
            "  [5.3985725e-04 1.3493896e-05 1.5943374e-02 ... 1.1722196e-06\n",
            "   1.5108630e-03 1.1318431e-03]\n",
            "  ...\n",
            "  [9.9997568e-01 2.6637773e-10 4.9652181e-06 ... 7.2538668e-09\n",
            "   5.2534510e-10 6.0773830e-10]\n",
            "  [9.9995637e-01 7.3077705e-10 5.6846966e-06 ... 1.8943034e-08\n",
            "   7.8513168e-10 4.5760179e-10]\n",
            "  [9.9992418e-01 1.6064846e-09 6.5356821e-06 ... 3.8041193e-08\n",
            "   1.1301586e-09 3.9944198e-10]]] (2, 271, 47)\n",
            "['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 271, 128)          1296768   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 271, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 271, 47)           24111     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 271, 47)           0         \n",
            "=================================================================\n",
            "Total params: 2,109,359\n",
            "Trainable params: 2,109,359\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 2504 samples, validate on 627 samples\n",
            "Epoch 1/40\n",
            "2504/2504 [==============================] - 26s 10ms/step - loss: 1.2840 - acc: 0.8598 - ignore_accuracy: 0.0205 - val_loss: 0.3809 - val_acc: 0.9063 - val_ignore_accuracy: 0.1281\n",
            "Epoch 2/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.3339 - acc: 0.9084 - ignore_accuracy: 0.1153 - val_loss: 0.3189 - val_acc: 0.9059 - val_ignore_accuracy: 0.0898\n",
            "Epoch 3/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.3110 - acc: 0.9111 - ignore_accuracy: 0.1177 - val_loss: 0.3081 - val_acc: 0.9169 - val_ignore_accuracy: 0.1379\n",
            "Epoch 4/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.3015 - acc: 0.9177 - ignore_accuracy: 0.1349 - val_loss: 0.3003 - val_acc: 0.9178 - val_ignore_accuracy: 0.1399\n",
            "Epoch 5/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2938 - acc: 0.9180 - ignore_accuracy: 0.1359 - val_loss: 0.2928 - val_acc: 0.9180 - val_ignore_accuracy: 0.1407\n",
            "Epoch 6/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.2856 - acc: 0.9182 - ignore_accuracy: 0.1377 - val_loss: 0.2846 - val_acc: 0.9189 - val_ignore_accuracy: 0.1503\n",
            "Epoch 7/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2784 - acc: 0.9208 - ignore_accuracy: 0.1640 - val_loss: 0.2785 - val_acc: 0.9222 - val_ignore_accuracy: 0.1807\n",
            "Epoch 8/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2736 - acc: 0.9230 - ignore_accuracy: 0.1841 - val_loss: 0.2740 - val_acc: 0.9226 - val_ignore_accuracy: 0.1844\n",
            "Epoch 9/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2694 - acc: 0.9233 - ignore_accuracy: 0.1870 - val_loss: 0.2713 - val_acc: 0.9238 - val_ignore_accuracy: 0.1981\n",
            "Epoch 10/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2654 - acc: 0.9256 - ignore_accuracy: 0.2116 - val_loss: 0.2677 - val_acc: 0.9299 - val_ignore_accuracy: 0.2624\n",
            "Epoch 11/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2613 - acc: 0.9295 - ignore_accuracy: 0.2523 - val_loss: 0.2627 - val_acc: 0.9284 - val_ignore_accuracy: 0.2468\n",
            "Epoch 12/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2539 - acc: 0.9337 - ignore_accuracy: 0.2967 - val_loss: 0.2564 - val_acc: 0.9350 - val_ignore_accuracy: 0.3166\n",
            "Epoch 13/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2442 - acc: 0.9389 - ignore_accuracy: 0.3531 - val_loss: 0.2439 - val_acc: 0.9401 - val_ignore_accuracy: 0.3710\n",
            "Epoch 14/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2289 - acc: 0.9441 - ignore_accuracy: 0.4088 - val_loss: 0.2244 - val_acc: 0.9442 - val_ignore_accuracy: 0.4143\n",
            "Epoch 15/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2048 - acc: 0.9487 - ignore_accuracy: 0.4574 - val_loss: 0.1987 - val_acc: 0.9499 - val_ignore_accuracy: 0.4749\n",
            "Epoch 16/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.1775 - acc: 0.9546 - ignore_accuracy: 0.5192 - val_loss: 0.1720 - val_acc: 0.9553 - val_ignore_accuracy: 0.5316\n",
            "Epoch 17/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.1512 - acc: 0.9609 - ignore_accuracy: 0.5868 - val_loss: 0.1486 - val_acc: 0.9597 - val_ignore_accuracy: 0.5777\n",
            "Epoch 18/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.1289 - acc: 0.9667 - ignore_accuracy: 0.6490 - val_loss: 0.1296 - val_acc: 0.9652 - val_ignore_accuracy: 0.6356\n",
            "Epoch 19/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.1103 - acc: 0.9723 - ignore_accuracy: 0.7070 - val_loss: 0.1141 - val_acc: 0.9705 - val_ignore_accuracy: 0.6913\n",
            "Epoch 20/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0946 - acc: 0.9772 - ignore_accuracy: 0.7602 - val_loss: 0.1006 - val_acc: 0.9739 - val_ignore_accuracy: 0.7280\n",
            "Epoch 21/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0803 - acc: 0.9815 - ignore_accuracy: 0.8058 - val_loss: 0.0882 - val_acc: 0.9784 - val_ignore_accuracy: 0.7736\n",
            "Epoch 22/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.0679 - acc: 0.9854 - ignore_accuracy: 0.8464 - val_loss: 0.0773 - val_acc: 0.9816 - val_ignore_accuracy: 0.8071\n",
            "Epoch 23/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0570 - acc: 0.9886 - ignore_accuracy: 0.8796 - val_loss: 0.0682 - val_acc: 0.9844 - val_ignore_accuracy: 0.8374\n",
            "Epoch 24/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0476 - acc: 0.9909 - ignore_accuracy: 0.9045 - val_loss: 0.0607 - val_acc: 0.9863 - val_ignore_accuracy: 0.8577\n",
            "Epoch 25/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0398 - acc: 0.9924 - ignore_accuracy: 0.9199 - val_loss: 0.0546 - val_acc: 0.9875 - val_ignore_accuracy: 0.8696\n",
            "Epoch 26/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0335 - acc: 0.9934 - ignore_accuracy: 0.9309 - val_loss: 0.0497 - val_acc: 0.9884 - val_ignore_accuracy: 0.8790\n",
            "Epoch 27/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0286 - acc: 0.9943 - ignore_accuracy: 0.9403 - val_loss: 0.0459 - val_acc: 0.9892 - val_ignore_accuracy: 0.8867\n",
            "Epoch 28/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0246 - acc: 0.9950 - ignore_accuracy: 0.9475 - val_loss: 0.0438 - val_acc: 0.9895 - val_ignore_accuracy: 0.8913\n",
            "Epoch 29/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0215 - acc: 0.9957 - ignore_accuracy: 0.9543 - val_loss: 0.0422 - val_acc: 0.9899 - val_ignore_accuracy: 0.8977\n",
            "Epoch 30/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0190 - acc: 0.9961 - ignore_accuracy: 0.9593 - val_loss: 0.0393 - val_acc: 0.9905 - val_ignore_accuracy: 0.9011\n",
            "Epoch 31/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0170 - acc: 0.9966 - ignore_accuracy: 0.9638 - val_loss: 0.0381 - val_acc: 0.9909 - val_ignore_accuracy: 0.9042\n",
            "Epoch 32/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0152 - acc: 0.9969 - ignore_accuracy: 0.9674 - val_loss: 0.0368 - val_acc: 0.9911 - val_ignore_accuracy: 0.9062\n",
            "Epoch 33/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0139 - acc: 0.9971 - ignore_accuracy: 0.9699 - val_loss: 0.0384 - val_acc: 0.9906 - val_ignore_accuracy: 0.9057\n",
            "Epoch 34/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0127 - acc: 0.9974 - ignore_accuracy: 0.9722 - val_loss: 0.0355 - val_acc: 0.9914 - val_ignore_accuracy: 0.9093\n",
            "Epoch 35/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0116 - acc: 0.9976 - ignore_accuracy: 0.9748 - val_loss: 0.0354 - val_acc: 0.9912 - val_ignore_accuracy: 0.9088\n",
            "Epoch 36/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0107 - acc: 0.9978 - ignore_accuracy: 0.9765 - val_loss: 0.0344 - val_acc: 0.9915 - val_ignore_accuracy: 0.9112\n",
            "Epoch 37/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0100 - acc: 0.9980 - ignore_accuracy: 0.9784 - val_loss: 0.0357 - val_acc: 0.9913 - val_ignore_accuracy: 0.9100\n",
            "Epoch 38/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0093 - acc: 0.9981 - ignore_accuracy: 0.9797 - val_loss: 0.0343 - val_acc: 0.9915 - val_ignore_accuracy: 0.9109\n",
            "Epoch 39/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0087 - acc: 0.9982 - ignore_accuracy: 0.9806 - val_loss: 0.0337 - val_acc: 0.9917 - val_ignore_accuracy: 0.9132\n",
            "Epoch 40/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0081 - acc: 0.9983 - ignore_accuracy: 0.9822 - val_loss: 0.0337 - val_acc: 0.9918 - val_ignore_accuracy: 0.9137\n",
            "['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 271, 128)          1296768   \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 271, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 271, 47)           24111     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 271, 47)           0         \n",
            "=================================================================\n",
            "Total params: 2,109,359\n",
            "Trainable params: 2,109,359\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n",
            "Train on 2504 samples, validate on 627 samples\n",
            "Epoch 1/40\n",
            "2504/2504 [==============================] - 27s 11ms/step - loss: 1.3425 - acc: 0.8596 - val_loss: 0.4073 - val_acc: 0.9058\n",
            "Epoch 2/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.3491 - acc: 0.9060 - val_loss: 0.3236 - val_acc: 0.9052\n",
            "Epoch 3/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.3148 - acc: 0.9095 - val_loss: 0.3104 - val_acc: 0.9158\n",
            "Epoch 4/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.3030 - acc: 0.9172 - val_loss: 0.3003 - val_acc: 0.9178\n",
            "Epoch 5/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.2932 - acc: 0.9174 - val_loss: 0.2909 - val_acc: 0.9175\n",
            "Epoch 6/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2862 - acc: 0.9177 - val_loss: 0.2852 - val_acc: 0.9190\n",
            "Epoch 7/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2811 - acc: 0.9184 - val_loss: 0.2810 - val_acc: 0.9202\n",
            "Epoch 8/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2753 - acc: 0.9211 - val_loss: 0.2769 - val_acc: 0.9231\n",
            "Epoch 9/40\n",
            "2504/2504 [==============================] - 25s 10ms/step - loss: 0.2702 - acc: 0.9232 - val_loss: 0.2726 - val_acc: 0.9216\n",
            "Epoch 10/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.2694 - acc: 0.9278 - val_loss: 0.2658 - val_acc: 0.9301\n",
            "Epoch 11/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.2580 - acc: 0.9354 - val_loss: 0.2587 - val_acc: 0.9368\n",
            "Epoch 12/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.2475 - acc: 0.9420 - val_loss: 0.2441 - val_acc: 0.9438\n",
            "Epoch 13/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2301 - acc: 0.9474 - val_loss: 0.2214 - val_acc: 0.9495\n",
            "Epoch 14/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.2048 - acc: 0.9509 - val_loss: 0.1951 - val_acc: 0.9504\n",
            "Epoch 15/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.1771 - acc: 0.9533 - val_loss: 0.1691 - val_acc: 0.9539\n",
            "Epoch 16/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.1512 - acc: 0.9594 - val_loss: 0.1466 - val_acc: 0.9610\n",
            "Epoch 17/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.1287 - acc: 0.9662 - val_loss: 0.1269 - val_acc: 0.9659\n",
            "Epoch 18/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.1086 - acc: 0.9724 - val_loss: 0.1091 - val_acc: 0.9722\n",
            "Epoch 19/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0907 - acc: 0.9789 - val_loss: 0.0941 - val_acc: 0.9772\n",
            "Epoch 20/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.0752 - acc: 0.9833 - val_loss: 0.0813 - val_acc: 0.9812\n",
            "Epoch 21/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0619 - acc: 0.9872 - val_loss: 0.0703 - val_acc: 0.9842\n",
            "Epoch 22/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.0508 - acc: 0.9901 - val_loss: 0.0617 - val_acc: 0.9868\n",
            "Epoch 23/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.0419 - acc: 0.9920 - val_loss: 0.0547 - val_acc: 0.9880\n",
            "Epoch 24/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.0348 - acc: 0.9933 - val_loss: 0.0498 - val_acc: 0.9888\n",
            "Epoch 25/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0294 - acc: 0.9943 - val_loss: 0.0458 - val_acc: 0.9896\n",
            "Epoch 26/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0251 - acc: 0.9951 - val_loss: 0.0428 - val_acc: 0.9902\n",
            "Epoch 27/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.0217 - acc: 0.9957 - val_loss: 0.0407 - val_acc: 0.9909\n",
            "Epoch 28/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0191 - acc: 0.9962 - val_loss: 0.0390 - val_acc: 0.9911\n",
            "Epoch 29/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0169 - acc: 0.9966 - val_loss: 0.0375 - val_acc: 0.9913\n",
            "Epoch 30/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0151 - acc: 0.9969 - val_loss: 0.0361 - val_acc: 0.9916\n",
            "Epoch 31/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.0136 - acc: 0.9972 - val_loss: 0.0349 - val_acc: 0.9919\n",
            "Epoch 32/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0123 - acc: 0.9975 - val_loss: 0.0345 - val_acc: 0.9919\n",
            "Epoch 33/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0112 - acc: 0.9977 - val_loss: 0.0338 - val_acc: 0.9920\n",
            "Epoch 34/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.0103 - acc: 0.9979 - val_loss: 0.0339 - val_acc: 0.9919\n",
            "Epoch 35/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0096 - acc: 0.9980 - val_loss: 0.0328 - val_acc: 0.9922\n",
            "Epoch 36/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0089 - acc: 0.9981 - val_loss: 0.0330 - val_acc: 0.9921\n",
            "Epoch 37/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.0324 - val_acc: 0.9923\n",
            "Epoch 38/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.0077 - acc: 0.9984 - val_loss: 0.0324 - val_acc: 0.9922\n",
            "Epoch 39/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0072 - acc: 0.9985 - val_loss: 0.0321 - val_acc: 0.9923\n",
            "Epoch 40/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0067 - acc: 0.9986 - val_loss: 0.0320 - val_acc: 0.9922\n",
            "783/783 [==============================] - 11s 14ms/step\n",
            "acc: 99.14417568294482\n",
            "[['running', 'is', 'very', 'important', 'for', 'me', '.'], ['I', 'was', 'running', 'every', 'day', 'for', 'a', 'month', '.']]\n",
            "[[5532 6077 2986 6833 4641 2005 6968    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [6126  523 5532 4937    6 4641 8214 3962 6968    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]]\n",
            "[[[7.6497984e-03 1.2199351e-05 3.9490159e-03 ... 3.0398587e-05\n",
            "   1.6072743e-02 1.1723328e-01]\n",
            "  [4.8760248e-06 5.9883541e-04 3.1738798e-04 ... 2.1636517e-07\n",
            "   3.9710483e-04 2.8452087e-07]\n",
            "  [3.4885545e-06 1.4495928e-05 4.1890754e-03 ... 4.9735604e-08\n",
            "   1.8628100e-04 2.3668996e-05]\n",
            "  ...\n",
            "  [9.9997234e-01 7.3920904e-12 2.3275203e-12 ... 3.3408798e-09\n",
            "   7.8206490e-11 6.6029514e-08]\n",
            "  [9.9994850e-01 2.1943983e-11 3.8433913e-12 ... 8.0615665e-09\n",
            "   1.5832760e-10 5.7638722e-08]\n",
            "  [9.9990916e-01 5.6667088e-11 6.4881056e-12 ... 1.6304277e-08\n",
            "   3.0497402e-10 5.1728488e-08]]\n",
            "\n",
            " [[5.2889500e-06 3.3156717e-05 2.2302123e-03 ... 3.4498217e-07\n",
            "   4.6369419e-03 3.0390788e-07]\n",
            "  [1.3782299e-07 2.8069046e-06 1.5248629e-02 ... 1.9837833e-08\n",
            "   5.1130402e-05 5.1455448e-11]\n",
            "  [3.4450572e-05 8.9096529e-06 1.0419391e-03 ... 4.0150394e-07\n",
            "   1.7563149e-04 1.7416261e-05]\n",
            "  ...\n",
            "  [9.9997234e-01 7.4100335e-12 2.3246366e-12 ... 3.3423009e-09\n",
            "   7.8092616e-11 6.5942039e-08]\n",
            "  [9.9994850e-01 2.1994937e-11 3.8388633e-12 ... 8.0648723e-09\n",
            "   1.5810790e-10 5.7565881e-08]\n",
            "  [9.9990904e-01 5.6792709e-11 6.4807955e-12 ... 1.6310622e-08\n",
            "   3.0456881e-10 5.1665868e-08]]] (2, 271, 47)\n",
            "['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 271, 128)          1296768   \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 271, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 271, 47)           24111     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 271, 47)           0         \n",
            "=================================================================\n",
            "Total params: 2,109,359\n",
            "Trainable params: 2,109,359\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 2504 samples, validate on 627 samples\n",
            "Epoch 1/40\n",
            "2504/2504 [==============================] - 27s 11ms/step - loss: 1.2875 - acc: 0.8599 - ignore_accuracy: 0.0320 - val_loss: 0.3971 - val_acc: 0.9075 - val_ignore_accuracy: 0.1413\n",
            "Epoch 2/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.3377 - acc: 0.9065 - ignore_accuracy: 0.0739 - val_loss: 0.3200 - val_acc: 0.9054 - val_ignore_accuracy: 0.1202\n",
            "Epoch 3/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.3111 - acc: 0.9131 - ignore_accuracy: 0.1246 - val_loss: 0.3073 - val_acc: 0.9171 - val_ignore_accuracy: 0.1391\n",
            "Epoch 4/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.3009 - acc: 0.9174 - ignore_accuracy: 0.1341 - val_loss: 0.2990 - val_acc: 0.9178 - val_ignore_accuracy: 0.1402\n",
            "Epoch 5/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2920 - acc: 0.9176 - ignore_accuracy: 0.1342 - val_loss: 0.2918 - val_acc: 0.9176 - val_ignore_accuracy: 0.1389\n",
            "Epoch 6/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2848 - acc: 0.9175 - ignore_accuracy: 0.1333 - val_loss: 0.2908 - val_acc: 0.9174 - val_ignore_accuracy: 0.1376\n",
            "Epoch 7/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2803 - acc: 0.9183 - ignore_accuracy: 0.1381 - val_loss: 0.2799 - val_acc: 0.9187 - val_ignore_accuracy: 0.1468\n",
            "Epoch 8/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2757 - acc: 0.9190 - ignore_accuracy: 0.1445 - val_loss: 0.2761 - val_acc: 0.9212 - val_ignore_accuracy: 0.1700\n",
            "Epoch 9/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2708 - acc: 0.9229 - ignore_accuracy: 0.1827 - val_loss: 0.2731 - val_acc: 0.9218 - val_ignore_accuracy: 0.1768\n",
            "Epoch 10/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2657 - acc: 0.9272 - ignore_accuracy: 0.2284 - val_loss: 0.2665 - val_acc: 0.9304 - val_ignore_accuracy: 0.2679\n",
            "Epoch 11/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2589 - acc: 0.9338 - ignore_accuracy: 0.2990 - val_loss: 0.2588 - val_acc: 0.9361 - val_ignore_accuracy: 0.3291\n",
            "Epoch 12/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2492 - acc: 0.9397 - ignore_accuracy: 0.3618 - val_loss: 0.2470 - val_acc: 0.9416 - val_ignore_accuracy: 0.3869\n",
            "Epoch 13/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2346 - acc: 0.9438 - ignore_accuracy: 0.4050 - val_loss: 0.2299 - val_acc: 0.9447 - val_ignore_accuracy: 0.4197\n",
            "Epoch 14/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2148 - acc: 0.9466 - ignore_accuracy: 0.4345 - val_loss: 0.2085 - val_acc: 0.9491 - val_ignore_accuracy: 0.4669\n",
            "Epoch 15/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.1927 - acc: 0.9515 - ignore_accuracy: 0.4875 - val_loss: 0.1866 - val_acc: 0.9532 - val_ignore_accuracy: 0.5101\n",
            "Epoch 16/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.1706 - acc: 0.9561 - ignore_accuracy: 0.5356 - val_loss: 0.1652 - val_acc: 0.9562 - val_ignore_accuracy: 0.5412\n",
            "Epoch 17/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.1486 - acc: 0.9601 - ignore_accuracy: 0.5785 - val_loss: 0.1452 - val_acc: 0.9612 - val_ignore_accuracy: 0.5947\n",
            "Epoch 18/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.1276 - acc: 0.9668 - ignore_accuracy: 0.6494 - val_loss: 0.1267 - val_acc: 0.9670 - val_ignore_accuracy: 0.6559\n",
            "Epoch 19/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.1076 - acc: 0.9733 - ignore_accuracy: 0.7176 - val_loss: 0.1081 - val_acc: 0.9723 - val_ignore_accuracy: 0.7112\n",
            "Epoch 20/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0887 - acc: 0.9795 - ignore_accuracy: 0.7832 - val_loss: 0.0922 - val_acc: 0.9782 - val_ignore_accuracy: 0.7718\n",
            "Epoch 21/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0718 - acc: 0.9847 - ignore_accuracy: 0.8381 - val_loss: 0.0780 - val_acc: 0.9824 - val_ignore_accuracy: 0.8155\n",
            "Epoch 22/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0576 - acc: 0.9884 - ignore_accuracy: 0.8769 - val_loss: 0.0668 - val_acc: 0.9853 - val_ignore_accuracy: 0.8465\n",
            "Epoch 23/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0466 - acc: 0.9908 - ignore_accuracy: 0.9031 - val_loss: 0.0582 - val_acc: 0.9866 - val_ignore_accuracy: 0.8601\n",
            "Epoch 24/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0380 - acc: 0.9924 - ignore_accuracy: 0.9199 - val_loss: 0.0523 - val_acc: 0.9878 - val_ignore_accuracy: 0.8737\n",
            "Epoch 25/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0317 - acc: 0.9936 - ignore_accuracy: 0.9330 - val_loss: 0.0476 - val_acc: 0.9886 - val_ignore_accuracy: 0.8808\n",
            "Epoch 26/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0268 - acc: 0.9945 - ignore_accuracy: 0.9421 - val_loss: 0.0440 - val_acc: 0.9894 - val_ignore_accuracy: 0.8891\n",
            "Epoch 27/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0232 - acc: 0.9952 - ignore_accuracy: 0.9492 - val_loss: 0.0418 - val_acc: 0.9898 - val_ignore_accuracy: 0.8931\n",
            "Epoch 28/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0203 - acc: 0.9958 - ignore_accuracy: 0.9558 - val_loss: 0.0393 - val_acc: 0.9904 - val_ignore_accuracy: 0.9002\n",
            "Epoch 29/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0179 - acc: 0.9963 - ignore_accuracy: 0.9608 - val_loss: 0.0380 - val_acc: 0.9906 - val_ignore_accuracy: 0.9015\n",
            "Epoch 30/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0159 - acc: 0.9967 - ignore_accuracy: 0.9652 - val_loss: 0.0363 - val_acc: 0.9911 - val_ignore_accuracy: 0.9073\n",
            "Epoch 31/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0143 - acc: 0.9971 - ignore_accuracy: 0.9689 - val_loss: 0.0351 - val_acc: 0.9913 - val_ignore_accuracy: 0.9088\n",
            "Epoch 32/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0128 - acc: 0.9973 - ignore_accuracy: 0.9715 - val_loss: 0.0343 - val_acc: 0.9914 - val_ignore_accuracy: 0.9100\n",
            "Epoch 33/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0116 - acc: 0.9976 - ignore_accuracy: 0.9747 - val_loss: 0.0336 - val_acc: 0.9915 - val_ignore_accuracy: 0.9109\n",
            "Epoch 34/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0106 - acc: 0.9978 - ignore_accuracy: 0.9769 - val_loss: 0.0328 - val_acc: 0.9918 - val_ignore_accuracy: 0.9136\n",
            "Epoch 35/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0096 - acc: 0.9980 - ignore_accuracy: 0.9790 - val_loss: 0.0326 - val_acc: 0.9918 - val_ignore_accuracy: 0.9139\n",
            "Epoch 36/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0089 - acc: 0.9982 - ignore_accuracy: 0.9812 - val_loss: 0.0319 - val_acc: 0.9919 - val_ignore_accuracy: 0.9156\n",
            "Epoch 37/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0082 - acc: 0.9983 - ignore_accuracy: 0.9824 - val_loss: 0.0316 - val_acc: 0.9920 - val_ignore_accuracy: 0.9168\n",
            "Epoch 38/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0076 - acc: 0.9985 - ignore_accuracy: 0.9840 - val_loss: 0.0316 - val_acc: 0.9920 - val_ignore_accuracy: 0.9166\n",
            "Epoch 39/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0070 - acc: 0.9986 - ignore_accuracy: 0.9851 - val_loss: 0.0315 - val_acc: 0.9920 - val_ignore_accuracy: 0.9167\n",
            "Epoch 40/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0066 - acc: 0.9987 - ignore_accuracy: 0.9863 - val_loss: 0.0312 - val_acc: 0.9921 - val_ignore_accuracy: 0.9177\n",
            "['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H3_GhSegXEWG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71f67427-9531-4265-9191-7975e7f01ebb"
      },
      "cell_type": "code",
      "source": [
        "model.save_weights(\"weights.h5\")\n",
        "model.save('model.h5')\n",
        "print(\"Saved model to disk\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XBWZ_COF_rsY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3709
        },
        "outputId": "d20ea40a-9df4-489a-c64f-fe25ac154e65"
      },
      "cell_type": "code",
      "source": [
        " \n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('treebank')\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        " \n",
        "#print(tagged_sentences[0])\n",
        "#print(\"Tagged sentences: \", len(tagged_sentences))\n",
        "#print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))\n",
        " \n",
        "sentences, sentence_tags =[], [] \n",
        "for tagged_sentence in tagged_sentences:\n",
        "    sentence, tags = zip(*tagged_sentence)\n",
        "    sentences.append(np.array(sentence))\n",
        "    sentence_tags.append(np.array(tags))\n",
        "\n",
        "print(sentences[5])\n",
        "print(sentence_tags[5])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        " \n",
        "(train_sentences, test_sentences, train_tags, test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)\n",
        "\n",
        "words, tags = set([]), set([])\n",
        " \n",
        "for s in train_sentences:\n",
        "    for w in s:\n",
        "        words.add(w.lower())\n",
        " \n",
        "for ts in train_tags:\n",
        "    for t in ts:\n",
        "        tags.add(t)\n",
        " \n",
        "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0  # The special value used for padding\n",
        "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
        " \n",
        "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
        "tag2index['-PAD-'] = 0\n",
        "\n",
        "\n",
        "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        " \n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])\n",
        "\n",
        "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "print(MAX_LENGTH)\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])\n",
        " \n",
        "        token_sequences.append(token_sequence)\n",
        " \n",
        "    return token_sequence\n",
        "\n",
        "test_samples = [\n",
        "    \"running is very important for me .\".split(),\n",
        "    \"I was running every day for a month .\".split()\n",
        "]\n",
        "print(test_samples)\n",
        "\n",
        "test_samples_X = []\n",
        "for s in test_samples:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        "    test_samples_X.append(s_int)\n",
        " \n",
        "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "print(test_samples_X)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "model.summary()\n",
        "\n",
        "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)\n",
        "\n",
        "predictions = model.predict(test_samples_X)\n",
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))\n",
        " "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "['Lorillard' 'Inc.' ',' 'the' 'unit' 'of' 'New' 'York-based' 'Loews'\n",
            " 'Corp.' 'that' '*T*-2' 'makes' 'Kent' 'cigarettes' ',' 'stopped' 'using'\n",
            " 'crocidolite' 'in' 'its' 'Micronite' 'cigarette' 'filters' 'in' '1956'\n",
            " '.']\n",
            "['NNP' 'NNP' ',' 'DT' 'NN' 'IN' 'JJ' 'JJ' 'NNP' 'NNP' 'WDT' '-NONE-' 'VBZ'\n",
            " 'NNP' 'NNS' ',' 'VBD' 'VBG' 'NN' 'IN' 'PRP$' 'NN' 'NN' 'NNS' 'IN' 'CD'\n",
            " '.']\n",
            "[814, 2872, 1338, 9383, 8715, 5299, 8473, 2736, 7536, 1874, 4572, 2736, 8147, 5438, 7536, 6299, 7421, 8473, 2736, 7536, 1874, 6057, 2736, 8147, 5438, 6902]\n",
            "[9399, 6509, 1469, 358, 6537, 4978, 7312, 4873, 9399, 3238, 1, 6902]\n",
            "[46, 20, 29, 46, 1, 29, 29, 46, 36, 27, 29, 46, 19, 46, 36, 34, 29, 29, 46, 36, 27, 29, 46, 19, 46, 11]\n",
            "[19, 38, 42, 20, 46, 30, 30, 27, 19, 30, 30, 11]\n",
            "271\n",
            "[ 814 2872 1338 9383 8715 5299 8473 2736 7536 1874 4572 2736 8147 5438\n",
            " 7536 6299 7421 8473 2736 7536 1874 6057 2736 8147 5438 6902    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[9399 6509 1469  358 6537 4978 7312 4873 9399 3238    1 6902    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[46 20 29 46  1 29 29 46 36 27 29 46 19 46 36 34 29 29 46 36 27 29 46 19\n",
            " 46 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n",
            "[19 38 42 20 46 30 30 27 19 30 30 11  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n",
            "[['running', 'is', 'very', 'important', 'for', 'me', '.'], ['I', 'was', 'running', 'every', 'day', 'for', 'a', 'month', '.']]\n",
            "[[5489 6021 2980 6766 4614 2004 6902    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [6069  522 5489 4903    5 4614 8147 3944 6902    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]]\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 271, 128)          1287424   \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 271, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 271, 47)           24111     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 271, 47)           0         \n",
            "=================================================================\n",
            "Total params: 2,100,015\n",
            "Trainable params: 2,100,015\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 2504 samples, validate on 627 samples\n",
            "Epoch 1/40\n",
            "2504/2504 [==============================] - 27s 11ms/step - loss: 1.2747 - acc: 0.8593 - val_loss: 0.3676 - val_acc: 0.9074\n",
            "Epoch 2/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.3332 - acc: 0.9134 - val_loss: 0.3161 - val_acc: 0.9072\n",
            "Epoch 3/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.3078 - acc: 0.9128 - val_loss: 0.3048 - val_acc: 0.9169\n",
            "Epoch 4/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2978 - acc: 0.9179 - val_loss: 0.2969 - val_acc: 0.9173\n",
            "Epoch 5/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2901 - acc: 0.9180 - val_loss: 0.2897 - val_acc: 0.9173\n",
            "Epoch 6/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2829 - acc: 0.9186 - val_loss: 0.2830 - val_acc: 0.9204\n",
            "Epoch 7/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2751 - acc: 0.9226 - val_loss: 0.2758 - val_acc: 0.9227\n",
            "Epoch 8/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2681 - acc: 0.9240 - val_loss: 0.2696 - val_acc: 0.9236\n",
            "Epoch 9/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2612 - acc: 0.9265 - val_loss: 0.2628 - val_acc: 0.9261\n",
            "Epoch 10/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2515 - acc: 0.9335 - val_loss: 0.2489 - val_acc: 0.9360\n",
            "Epoch 11/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.2350 - acc: 0.9414 - val_loss: 0.2283 - val_acc: 0.9428\n",
            "Epoch 12/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.2091 - acc: 0.9472 - val_loss: 0.1997 - val_acc: 0.9482\n",
            "Epoch 13/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.1788 - acc: 0.9537 - val_loss: 0.1710 - val_acc: 0.9541\n",
            "Epoch 14/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.1506 - acc: 0.9602 - val_loss: 0.1464 - val_acc: 0.9606\n",
            "Epoch 15/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.1255 - acc: 0.9675 - val_loss: 0.1247 - val_acc: 0.9677\n",
            "Epoch 16/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.1043 - acc: 0.9742 - val_loss: 0.1064 - val_acc: 0.9730\n",
            "Epoch 17/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0858 - acc: 0.9805 - val_loss: 0.0919 - val_acc: 0.9771\n",
            "Epoch 18/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.0703 - acc: 0.9850 - val_loss: 0.0798 - val_acc: 0.9810\n",
            "Epoch 19/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0574 - acc: 0.9882 - val_loss: 0.0694 - val_acc: 0.9840\n",
            "Epoch 20/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0468 - acc: 0.9909 - val_loss: 0.0616 - val_acc: 0.9858\n",
            "Epoch 21/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0386 - acc: 0.9926 - val_loss: 0.0554 - val_acc: 0.9872\n",
            "Epoch 22/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0322 - acc: 0.9938 - val_loss: 0.0509 - val_acc: 0.9878\n",
            "Epoch 23/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0272 - acc: 0.9947 - val_loss: 0.0473 - val_acc: 0.9885\n",
            "Epoch 24/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.0233 - acc: 0.9954 - val_loss: 0.0448 - val_acc: 0.9890\n",
            "Epoch 25/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0204 - acc: 0.9960 - val_loss: 0.0428 - val_acc: 0.9893\n",
            "Epoch 26/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0180 - acc: 0.9965 - val_loss: 0.0411 - val_acc: 0.9898\n",
            "Epoch 27/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0159 - acc: 0.9968 - val_loss: 0.0399 - val_acc: 0.9901\n",
            "Epoch 28/40\n",
            "2504/2504 [==============================] - 24s 10ms/step - loss: 0.0142 - acc: 0.9972 - val_loss: 0.0394 - val_acc: 0.9902\n",
            "Epoch 29/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0129 - acc: 0.9974 - val_loss: 0.0383 - val_acc: 0.9904\n",
            "Epoch 30/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.0382 - val_acc: 0.9905\n",
            "Epoch 31/40\n",
            "2504/2504 [==============================] - 23s 9ms/step - loss: 0.0107 - acc: 0.9978 - val_loss: 0.0374 - val_acc: 0.9906\n",
            "Epoch 32/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0098 - acc: 0.9980 - val_loss: 0.0366 - val_acc: 0.9907\n",
            "Epoch 33/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0090 - acc: 0.9981 - val_loss: 0.0362 - val_acc: 0.9908\n",
            "Epoch 34/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0083 - acc: 0.9983 - val_loss: 0.0365 - val_acc: 0.9909\n",
            "Epoch 35/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0078 - acc: 0.9984 - val_loss: 0.0362 - val_acc: 0.9908\n",
            "Epoch 36/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0072 - acc: 0.9985 - val_loss: 0.0360 - val_acc: 0.9909\n",
            "Epoch 37/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0067 - acc: 0.9986 - val_loss: 0.0368 - val_acc: 0.9909\n",
            "Epoch 38/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0063 - acc: 0.9987 - val_loss: 0.0365 - val_acc: 0.9910\n",
            "Epoch 39/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0059 - acc: 0.9988 - val_loss: 0.0368 - val_acc: 0.9910\n",
            "Epoch 40/40\n",
            "2504/2504 [==============================] - 24s 9ms/step - loss: 0.0055 - acc: 0.9989 - val_loss: 0.0361 - val_acc: 0.9911\n",
            "['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7h0OdcrF_wJj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96be3147-3fb3-4e6a-c367-36968ab16a68"
      },
      "cell_type": "code",
      "source": [
        "model.save_weights(\"weights1.h5\")\n",
        "model.save('model1.h5')\n",
        "print(\"Saved model to disk\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U6ioYXXxXaYB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model1 = load_model('model1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nS2EYuD5XxX9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "203ab332-fd8d-47b3-ccfd-5a9599e6f371"
      },
      "cell_type": "code",
      "source": [
        "test_samples = [\n",
        "    \"my name is John Patel, Kandarp Kakkad, Chirag Bajaj .\".split(),\n",
        "]\n",
        "test_samples_X = []\n",
        "for s in test_samples:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        "    test_samples_X.append(s_int)\n",
        " \n",
        "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "print(test_samples)\n",
        "predictions = model1.predict(test_samples_X)\n",
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['my', 'name', 'is', 'John', 'Patel,', 'Kandarp', 'Kakkad,', 'Chirag', 'Bajaj', '.']]\n",
            "['PRP$', 'NN', 'VBZ', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iNEb6kbpHTf4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wAJVjvpQZjA4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}